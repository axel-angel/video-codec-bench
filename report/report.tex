\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb} % math symbols
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lastpage}

\usepackage{cite}
\usepackage{hyperref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\renewcommand{\contentsname}{Summary}

\lhead{Axel Angel \& Luca La Spada \& Guillaume Martres}
\rhead{Big data}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\today}

\begin{document}
%\input{./titlepage.tex}

\tableofcontents

\newpage

\section{Introduction}
% Project presentation and goals
New and emerging video codec standards like
HEVC/H.265~ \cite{hevc} (MPEG standard),
VP9~ \cite{vp9} (open, royalty-free codec
developed by Google) or Daala~ \cite{daala} (research
project developed in the open by Mozilla) have caused a great increase of
activity around open source video encoders for these formats. The main challenge
of any encoder is to provide the greatest visual quality at acceptable file size
and encoding speed. This means that all these projects must measure their output
quality and compare it with the state of the art. It would be a great boost to
productivity for developers if an open source project to compare and track the
quality of these evolving encoders existed. This is the goal of CodecWatch,
which provides the following functionalities:
\begin{itemize}
\item Do nightly encode of a standard set of video clips using the latest git
revision of various open source encoders, at various bitrates.
\item Since more encoded videos mean that we can do more thorough comparisons,
  and video encoding need a lot of CPU resources, the project need to support
  distributed encoding to an arbitrary number of machines.
\item Display in graph forms various metrics (like
    PSNR~ \cite{psnr} or
    SSIM~ \cite{ssim})
\item As quality metrics are only an approximation of the actual visual quality,
provide a way to compare the encoded videos side-by-side.
\end{itemize} Instead of starting from scratch, we implemented CodecWatch by
extending the existing open source encoding platform
OSCIED~ \cite{oscied}.

\section{OSCIED description}
% Why we choose oscied, architecture summary
The goal of OSCIED is to provide a distributed cloud-powered platform to encode,
publish and manage distributed resources.  In practice it means that medias are
first uploaded to the platform, then multiple encoding jobs can be dispatched to
transformer machines and done in parallel.  The resulting medias are then
available in the platform for further processing or direct download by the
administrator.  Moreover these files can be published on any publisher machine
for public access.
% TODO: develop more
For more details, we invite the reader to consult the original paper by David
Fischer~\cite{df_thesis}.

% Explain we had to understand how OSCIED works, architecture complexity, it's enormous

\section{Deployment}
\subsection{OSCIED}
% what OSCIED provides to deploy, locally works fine
As the goal of OSCIED is to distribute computer resources across multiple
machines, the platform provides high-level scripts to deploy the different roles
locally or to a cloud provider.  Behind the scene OSCIED uses juju, a
provisioning and deployment tool that supports major cloud providers.  We won't
describe in detail how juju works but basically it is made of two parts: the
internals capable of interfacing with the cloud APIs and lots of
role-specific files (charms).  The charms are in fact description files which
tells the system how to deploy the different roles: package dependencies and how
to startup the services.  For example the webui of OSCIED needs a web server
thus the webui charm will tell juju to install Apache with PHP and setup the web
files in the right directory.

We first began to deploy a new instance of OSCIED with all roles locally on our
server at EPFL, each role inside its own LXC (linux container).  This involved
numerous bug fixes to OSCIED which where then merged upstream, see
\cite{ebu_merge1},
\cite{oscied_merge1}
and
\cite{pytoolbox_merge1}.

\subsection{Use of Microsoft Azure}
As video encoding is a very CPU intensive task, we had planned to use our
provided Azure instances to increase the number of videos and encoders we could
demonstrate our project on. Azure support required us to use a recent version of
Ubuntu and thus update OSCIED to account for changes in its dependencies (like
the removal of the -sameq argument from ffmpeg). This was completed
successfully, but unfortunately, we did not manage to properly deploy OSCIED on
Azure on time. This is due mostly to the quite complex and fragile scripts that
are normally used to deploy OSCIED and bugs in juju itself, like
\cite{juju_bug1} which we reported ourselves.

%\section{Adaptations}
% modification we made, how we changed

\subsection{Functional modifications of OSCIED}
% TODO: Axel
To integrate our goals into OSCIED we decided to develop directly into the live
containers, we had direct feedback of our modifications.  Multiple roles were
affected: Orchestra, Transformer and Webui.  We needed to collect statistics
such as the video bitrate, quality measures such as PSNR/SSIM, media source such
as the git repository and commit, directly into the stored metadata of the media
at encoding time (these are pushed into MongoDB on the Orchestra role after
completion)

We first had to understand the internal implementation: hierarchy of files, the
classes and how they all work together.  This took us some time and finally we
were able to modify exactly what we wanted.  There were multiple obstacles due
to the dynamic nature of Python such as multiple level of indirections of the
API (callbacks) and the lack of type description, these components are lazily
coupled and they break only at runtime after a job.  Thus we had to debug our
code in live by looking at the logged files and iterate until we had what we
wanted.

First we decided to modify the Transformer code where the encoding process is
triggered after which the metadata are computed.  It is written in Python and
backed up by Celery (a task queueing library), thus the call are all
asynchronous and a callback is used.  We modified the encoding target (ffmpeg)
to collect more metadata (called measures), these are computed by specialized
programs of the Daala project.

Secondly we had to modify Orchestra to accept these measures and put it as
regular metadata into the database. Some part of the code base is shared with
the Transformer and thus are written in Python but backed up by Flask (a light
server handful for serving APIs).

Thirdly the web interface Webui had to modified to display these informations,
this part is written in PHP.  Columns were added in the media panel of the
interface in order to show the results of our tests and for the final users so
that they can compare numeric values directly.  As textual information is not
sufficient, we wrote a small API that serves these measures directly to a
separate interface with graphs we wrote (see below).  We had to integrate our
work with the PHP framework of the Webui (called CodeIgniter), that is: register
new routes and ensure we provide a json output correctly to interpolate with
Javascript (see the next section).

\section{Interface Additions}
At this point, the statistics were only viewable through a big table.  Hence, we
created two new pages: one with graphs to plot them and one to visually compare
two videos of different configurations.

\subsection{Statistics interface}
The first page contains a graph which greatly helps to understand how encoders
behave.  The graph is a plot that shows you the correlation of the quality of
different encoders in term of our measures (PSNR/SSIM) with respect to the
bitrate.  These measures are retrieved from the Orchestra database through our
own API, they are made on specific videos (called samples).  The interface
allows you to choose what is plotted: which samples, encoders and metrics.
Moreover, you can define time interval, during which measures were done, to show
the evolution of the encoders as they are developed.  We expect them to improve
over time but there may be regressions which would be easily tracked down with
our graphs.

The interface is implemented mostly using HTML, Javascript and CSS.  We used
the following Javascript modules:
\begin{description}
    \item[HarvestHQ]\cite{harvesthq_chosen} Improve visually and functionally the \verb%<select>% html tag.
    \item[jQueryUI, TimePicker]\cite{timepicker} \cite{jqueryui}
      Replace the default \verb%datepicker% of jQueryUI with a more detailed one (you can set
      even the second)
    \item[jQuery, flotcharts]\cite{jquery} ~\cite{flotchart}
      jQuery plugin used to create plots.
\end{description}
Furthermore, we used some design ideas from
\href{http://areweslimyet.com}{areweslimyet}, as a source of inspiration.

In figure~\ref{fig:graph1}, you can see a demonstration of our interface.  The
graph plots the bitrate as the X-axis and the quality in dB as the Y-axis.  If you
hover a point with your cursor, a nice tooltip will appear with more detailed
informations for each point: such as exact bitrate, noise value and commit
related information.

\begin{figure}[!h] \centering
  \includegraphics[width=1\textwidth]{figures/graph1.png}
  \caption{Example of graph rendering}
  \label{fig:graph1}
\end{figure}

\subsection{SplitView}
While graphs are great to track the progress of encoders, the metrics used are
only an approximation of the human visual system, thus it makes sense to also
provide a way to visually compare the output of the encoders.
This is why we took the visual comparator SplitView~\cite{splitview}
(originally developed by one of our member) and adapted it to fit into our
interface.  The principle of SplitView is simple as you can
see~\ref{fig:split1}: two videos are rendered into the same area, thus they are
overlapping.  They are split in the middle by a vertical bar which can moved to
unveil part of the video of your choice.  Thus anyone can compare regions of the
same sample and inspect the visual differences due to the different encoding
settings. Every sample encoded by OSCIED can be watched.

While adapting SplitView for our use, we also added an important feature: the
\emph{Blind Test} mode which is enabled by the green button at the top of the
videos in figure~\ref{fig:split1}.  When it is clicked, the green box with
informations on the video is hidden and the two videos are swapped randomly. The goal
of this feature is to avoid any bias due to the user knowing prior information
about the two videos.  Thus the two videos, now shuffled, must be distinguished
only by their quality as the eye perceives it, and not by some prior judgement.
Of course you can show the video details once more by pressing the green button
again to reveal if you were right or not.
%Thanks to this feature you loose
%completely the notion of which would be the best one based on his intrinsic
%value. When you reclick on the blind test button, the solution reappears.

\begin{figure}[!h] \centering
  \includegraphics[width=1\textwidth]{figures/split1.png}
  \caption{SplitView}
  \label{fig:split1}
\end{figure}

\section{Future work}
Beside support for Windows Azure, there's a few possible improvements we could
have done if we had had enough time and resources (eg: more people):
\subsection{Chunked encoding}
The most efficient way to parallelize video encoding is to split the source into
chunks which each last a few seconds, encode these chunks separetly, then merge
them. This does not negatively affect the video quality since video streaming
requires that segments of more than a few seconds are independently decodable
anyway\footnote{otherwise seeking to a particular point in a video would be too
slow, since you would have to retrieve and decode everything before that point
in the video first}. CodecWatch does not implement this, although this wouldn't
be too hard to add\footnote{ffmpeg already supports chunking a video, and tools
like mkvtoolnix or l-smash can be used to merge it back together}. This is not a
critical problem because CodecWatch is mostly designed for benchmarking purposes
and this is usually done by using very short clips~\cite{derf} and thus chunking
would strongly defeat the purpose of most encoding technics that leverage video
continuity to decrease video size.
\subsection{Easier configuration of the nightly encodes}
Right now, it's easy for an user to change the set of videos that are used for
benchmarking by uploading or deleting videos in the web interface, but there is
no interface to change the encoding parameters used such as quality/bitrate,
instead the \verb%cron_enco.py% script has to be modified manually, this
requires direct access to the Orchestra machine. The modification wouldn't be
too hard to allow basic editing but we lacked the time to implement a well
integrated solution.

\section{Conclusion}
% what we made, sumup, how can be used and continued, improved
% what we did is useful, blabla
We managed to produce a useful proof of concept of what a benchmarking platform
for encoders should look like. While the use of OSCIED allowed us to avoid
having to design our own distributed encoding system, it turned out to be harder
to adapt to our needs that anticipated. Thus, it might make sense for future
work to rebuild the server side from scratch using modern deployment tools like
CoreOS~\cite{coreos} from scratch but keep the client side (the metrics graphs
and SplitView).
\appendix
\section{Installation guide notes}
OSCIED was tested on Ubuntu 14.04 with juju \verb%1.18.2%. One should read the
deployment guide\cite{oscied_deploy} (but with our repository) and take a look
at the thesis\cite{df_thesis}. We deployed locally in LXC and the following
steps should help anyone to install our system:
\begin{verbatim}
sudo apt-get install software-properties-common
sudo add-apt-repository ppa:juju/stable # juju repo

juju init # init config
juju switch local # switch to local instead of amazon deploy
juju bootstrap
git clone https://github.com/codecwatch/OSCIED
# suppose OSCIED is installed (following official guide):
(cd OSCIED/scripts; sh menu.sh install) # if not already done
(cd OSCIED/scripts; sh menu.sh deploy) # pick: Local
# y for first install of charms
# y for certificate per scenario
# y for transform (1 instance)
# y for publisher (1 instance)
# y for orchestra
# y for web-ui
# n for haproxy
# y for everything until "Initialize orchestra?"
# Then wait for everything to be deployed (use 'juju status'
# to see if anything is still pending)
# Then y
\end{verbatim}

\bibliography{report}{}
\bibliographystyle{plain}

\end{document}
